import os
import time
import random
import platform
import requests
import pandas as pd
import jieba
import matplotlib.pyplot as plt
from lxml import etree
from snownlp import SnowNLP
from wordcloud import WordCloud

# å¦‚æœæ²¡æœ‰ fake_useragent æŠ¥é”™ï¼Œå¯ä»¥åˆ é™¤è¿™è¡Œå¼•ç”¨ï¼Œç›´æ¥ç”¨ä¸‹é¢å†™æ­»çš„ Header
# from fake_useragent import UserAgent

# ================= âš ï¸ é…ç½®åŒºåŸŸ (å¿…æ”¹) âš ï¸ =================

# 1. ç”µå½±ID (åªå¡«æ•°å­—ï¼ä¸è¦å¡«ç½‘å€ï¼)
# ä¾‹å¦‚ï¼šã€Šè‚–ç”³å…‹çš„æ•‘èµã€‹IDæ˜¯ 1292052
MOVIE_ID = '1292052'

# 2. è±†ç“£Cookie (å¿…å¡«ï¼å¦åˆ™æ— æ³•çˆ¬å–ï¼Œç”šè‡³ç›´æ¥ 403)
# è·å–æ–¹æ³•ï¼šæµè§ˆå™¨ç™»å½•è±†ç“£ -> F12 -> Console -> è¾“å…¥ document.cookie -> å¤åˆ¶ç»“æœ
COOKIE = 'bid=lf7Bck0ksNo; ll="118337"; _gid=GA1.2.419664119.1770037444; _ga=GA1.1.1539301421.1770037444; _ga_Y4GN1R87RG=GS2.1.s1770037443$o1$g0$t1770037448$j55$l0$h0; dbcl2="285319578:wKGoR+2Zj/I"; ck=7ibz; ap_v=0,6.0; push_noty_num=0; push_doumail_num=0; frodotk_db="a5101fafc1d2c9b1ef305f0f45785d13"; __utma=30149280.1539301421.1770037444.1770037478.1770037478.1; __utmc=30149280; __utmz=30149280.1770037478.1.1.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utmv=30149280.28531; __utma=223695111.1539301421.1770037444.1770037975.1770037975.1; __utmb=223695111.0.10.1770037975; __utmc=223695111; __utmz=223695111.1770037975.1.1.utmcsr=douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/gallery/topic/3666789/; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1770037975%2C%22https%3A%2F%2Fwww.douban.com%2Fgallery%2Ftopic%2F3666789%2F%22%5D; _pk_id.100001.4cf6=8cf1ca59b4c1cd8e.1770037975.; _pk_ses.100001.4cf6=1; __yadk_uid=f4ZqNHPORbX1sqbg9OfXkmJURPUnbiV4; _vwo_uuid_v2=D61CC81D8B5F69405EC404CB35C4C2B3A|24ad3fe3d78e9fb3077f815835b4cd27; __utmt=1; __utmb=30149280.20.6.1770037963031'

# 3. å­—ä½“è®¾ç½® (å»ºè®®ä¸‹è½½ SimHei.ttf æ”¾åœ¨ä»£ç åŒç›®å½•ä¸‹)
LOCAL_FONT_NAME = 'SimHei.ttf'


# ========================================================

class FontManager:
    """è‡ªåŠ¨è§£å†³ä¸­æ–‡å­—ä½“è·¯å¾„ï¼Œé˜²æ­¢ä¹±ç """

    @staticmethod
    def get_font_path():
        # 1. ä¼˜å…ˆæ£€æµ‹æœ¬åœ°åŒçº§ç›®å½•
        if os.path.exists(LOCAL_FONT_NAME):
            return LOCAL_FONT_NAME

        system = platform.system()
        # 2. æ ¹æ®ç³»ç»Ÿå°è¯•è°ƒç”¨å†…ç½®å­—ä½“
        if system == 'Windows':
            paths = ['C:/Windows/Fonts/simhei.ttf', 'C:/Windows/Fonts/msyh.ttc']
            for p in paths:
                if os.path.exists(p): return p
        elif system == 'Darwin':  # Mac
            paths = ['/System/Library/Fonts/Supplemental/Arial Unicode.ttf', '/System/Library/Fonts/PingFang.ttc']
            for p in paths:
                if os.path.exists(p): return p

        print("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†ã€‚å»ºè®®ä¸‹è½½ SimHei.ttf")
        return 'arial.ttf'  # è‹±æ–‡é»˜è®¤å­—ä½“


class DoubanSpider:
    def __init__(self, movie_id, max_pages=3, cookie=''):
        self.movie_id = movie_id
        self.max_pages = max_pages
        # æ„é€  URL
        self.base_url = f'https://movie.douban.com/subject/{movie_id}/comments'

        # ä½¿ç”¨å›ºå®šçš„ Headerï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Cookie': cookie,
            'Host': 'movie.douban.com',
            'Connection': 'keep-alive'
        }
        self.data_list = []

    def fetch_page(self, start=0):
        params = {'start': start, 'limit': 20, 'status': 'P', 'sort': 'new_score'}
        try:
            # éšæœºä¼‘çœ 
            time.sleep(random.uniform(1.5, 3.0))

            # --- è°ƒè¯•æ‰“å°ï¼šç¡®è®¤ç½‘å€æ˜¯å¦æ­£ç¡® ---
            if start == 0:
                print(f"   [Debug] æ­£åœ¨è¯·æ±‚ç½‘å€: {self.base_url}")
            # --------------------------------

            response = requests.get(self.base_url, headers=self.headers, params=params, timeout=10)

            if response.status_code == 200:
                return response.text
            elif response.status_code == 404:
                print(f"âŒ é”™è¯¯ï¼š404 Not Foundã€‚")
                print(f"   åŸå› ï¼šç”µå½±ID [{self.movie_id}] é”™è¯¯ï¼Œæˆ–è€…è¯¥ç”µå½±ä¸å­˜åœ¨ã€‚")
                print("   æ£€æŸ¥ï¼šè¯·ç¡®ä¿ MOVIE_ID å˜é‡åªåŒ…å«æ•°å­—ï¼")
                return None
            elif response.status_code == 403:
                print("âŒ é”™è¯¯ï¼š403 Forbiddenã€‚")
                print("   åŸå› ï¼šCookieå¤±æ•ˆ æˆ– IPè¢«å°ã€‚è¯·æ›´æ–° Cookieï¼")
                return None
            else:
                print(f"âš ï¸ è¯·æ±‚çŠ¶æ€ç å¼‚å¸¸: {response.status_code}")
                return None

        except Exception as e:
            print(f"âŒ è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {e}")
            return None

    def parse_html(self, html):
        if not html: return

        # --- ğŸ› ï¸ è°ƒè¯•æ ¸å¿ƒä»£ç ï¼šä¿å­˜ç½‘é¡µçœ‹çœ‹åˆ°åº•æ˜¯å•¥ ---
        with open("debug_douban.html", "w", encoding="utf-8") as f:
            f.write(html)
        print("   [è°ƒè¯•] å·²å°†ä¸‹è½½çš„ç½‘é¡µä¿å­˜ä¸º 'debug_douban.html'ï¼Œè¯·åœ¨æ–‡ä»¶å¤¹ä¸­åŒå‡»æ‰“å¼€å®ƒï¼")
        # ----------------------------------------------

        tree = etree.HTML(html)

        # 1. æ£€æŸ¥æ ‡é¢˜
        title = tree.xpath('//title/text()')
        print(f"   [è°ƒè¯•] ç½‘é¡µæ ‡é¢˜æ˜¯: {title}")

        # 2. å°è¯•æ›´å®½æ¾çš„åŒ¹é…è§„åˆ™ (å…³é”®ä¿®æ”¹ï¼ï¼ï¼)
        # åŸæ¥çš„å†™æ³•ï¼š[@class="comment-item "] (å¿…é¡»å¸¦ç©ºæ ¼ï¼Œå¤ªæ­»æ¿)
        # ç°åœ¨çš„å†™æ³•ï¼š[contains(@class, "comment-item")] (åªè¦åŒ…å«è¿™ä¸ªè¯å°±è¡Œ)
        items = tree.xpath('//div[contains(@class, "comment-item")]')

        if not items:
            print("   âš ï¸ ä¾ç„¶æ²¡æœ‰æ‰¾åˆ°è¯„è®ºæ¡ç›®ã€‚è¯·æ£€æŸ¥ debug_douban.html æ˜¯å¦æ˜¾ç¤ºäº†ç™»å½•é¡µé¢ã€‚")
            return

        for item in items:
            try:
                # å…¼å®¹ä¸åŒæƒ…å†µçš„ç”¨æˆ·åæå–
                user_try = item.xpath('.//span[@class="comment-info"]/a/text()')
                username = user_try[0] if user_try else "æœªçŸ¥ç”¨æˆ·"

                rating_class = item.xpath('.//span[@class="comment-info"]/span[2]/@class')
                rating = int(rating_class[0].split('allstar')[1].split()[0]) // 10 if rating_class else 0

                content_list = item.xpath('.//span[@class="short"]/text()')
                content = content_list[0].strip() if content_list else ""

                if content:
                    self.data_list.append({
                        'user': username,
                        'rating': rating,
                        'content': content
                    })
            except Exception as e:
                # print(f"è§£æå•æ¡å‡ºé”™: {e}") # åªæœ‰è°ƒè¯•æ—¶æ‰æ‰“å¼€ï¼Œé¿å…åˆ·å±
                continue

    def run(self):
        print(f"ğŸ•·ï¸ å¼€å§‹é‡‡é›†ç”µå½±ID: [{self.movie_id}]")
        for page in range(self.max_pages):
            print(f"   æ­£åœ¨çˆ¬å–ç¬¬ {page + 1}/{self.max_pages} é¡µ...")
            html = self.fetch_page(start=page * 20)
            if not html: break  # å¦‚æœè¯·æ±‚å¤±è´¥ç›´æ¥é€€å‡º

            start_len = len(self.data_list)
            self.parse_html(html)
            end_len = len(self.data_list)

            if end_len == start_len:
                print("   âš ï¸ æœ¬é¡µæ²¡æœ‰æå–åˆ°æ–°æ•°æ®ï¼Œåœæ­¢ç¿»é¡µã€‚")
                break

        if not self.data_list:
            return pd.DataFrame()  # è¿”å›ç©ºè¡¨

        df = pd.DataFrame(self.data_list)
        df.drop_duplicates(subset=['content'], inplace=True)
        df.to_csv('douban_comments.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(df)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œå·²ä¿å­˜åˆ° CSVã€‚")
        return df


class NLPProcessor:
    def __init__(self, filepath='douban_comments.csv'):
        self.df = pd.read_csv(filepath)
        self.stopwords = {'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'åœ¨', 'ä¹Ÿ', 'éƒ½', 'å’Œ', 'å°±', 'äºº', 'çœ‹', 'ç”µå½±', 'è§‰å¾—', 'å°±æ˜¯',
                          'çœŸçš„'}

    def process(self):
        print("ğŸ§  å¼€å§‹ NLP åˆ†æ...")
        # 1. æƒ…æ„Ÿæ‰“åˆ†
        self.df['content'] = self.df['content'].astype(str)
        self.df['sentiment_score'] = self.df['content'].apply(lambda x: SnowNLP(x).sentiments if len(x) > 1 else 0.5)

        # 2. æ‰“æ ‡ç­¾
        self.df['sentiment_label'] = self.df['sentiment_score'].apply(
            lambda x: 'ç§¯æ' if x >= 0.6 else ('æ¶ˆæ' if x <= 0.4 else 'ä¸­æ€§')
        )

        # 3. åˆ†è¯
        def seg_words(text):
            return [w for w in jieba.cut(text) if w not in self.stopwords and len(w) > 1]

        self.df['words'] = self.df['content'].apply(seg_words)

        self.df.to_csv('douban_processed.csv', index=False, encoding='utf-8-sig')
        print("âœ… åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜ã€‚")
        return self.df


class Visualizer:
    def __init__(self, filepath='douban_processed.csv'):
        self.df = pd.read_csv(filepath)
        self.font_path = FontManager.get_font_path()
        # Matplotlib å­—ä½“è®¾ç½®
        try:
            from matplotlib import font_manager
            self.prop = font_manager.FontProperties(fname=self.font_path)
        except:
            self.prop = None

    def run_all(self):
        print("ğŸ“Š å¼€å§‹ç»˜å›¾...")

        # å›¾1ï¼šè¯„åˆ†åˆ†å¸ƒ
        plt.figure(figsize=(7, 5))
        counts = self.df['rating'].value_counts().sort_index()
        plt.bar(counts.index, counts.values, color='#3498DB')
        plt.title('ç”¨æˆ·è¯„åˆ†åˆ†å¸ƒ', fontproperties=self.prop, fontsize=14)
        plt.xlabel('æ˜Ÿçº§', fontproperties=self.prop)
        plt.savefig('rating_dist.png')
        print("   -> å·²ç”Ÿæˆ rating_dist.png")

        # å›¾2ï¼šæƒ…æ„Ÿå æ¯”
        plt.figure(figsize=(6, 6))
        sentiment_counts = self.df['sentiment_label'].value_counts()
        plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',
                colors=['#58D68D', '#EC7063', '#F5CBA7'],
                textprops={'fontproperties': self.prop})
        plt.title('æƒ…æ„Ÿå€¾å‘å æ¯”', fontproperties=self.prop, fontsize=14)
        plt.savefig('sentiment_pie.png')
        print("   -> å·²ç”Ÿæˆ sentiment_pie.png")

        # å›¾3ï¼šè¯äº‘
        # æ•°æ®å¤„ç†ï¼šå°†å­—ç¬¦ä¸²æ ¼å¼çš„åˆ—è¡¨è½¬å›åˆ—è¡¨
        all_words = []
        for w_list in self.df['words']:
            # CSVè¯»å–åå¯èƒ½æ˜¯å­—ç¬¦ä¸² "['a', 'b']"
            if isinstance(w_list, str):
                import ast
                try:
                    w_list = ast.literal_eval(w_list)
                except:
                    w_list = []
            if isinstance(w_list, list):
                all_words.extend(w_list)

        text = " ".join(all_words)
        if not text.strip():
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œè·³è¿‡è¯äº‘ç”Ÿæˆã€‚")
            return

        wc = WordCloud(
            font_path=self.font_path,
            background_color='white',
            width=800, height=600,
            max_words=80
        )
        wc.generate(text)
        plt.figure(figsize=(8, 6))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title('é«˜é¢‘å…³é”®è¯', fontproperties=self.prop, fontsize=14)
        plt.savefig('wordcloud.png')
        print("   -> å·²ç”Ÿæˆ wordcloud.png")


if __name__ == "__main__":
    # --- 1. çˆ¬è™«éƒ¨åˆ† ---
    spider = DoubanSpider(movie_id=MOVIE_ID, max_pages=3, cookie=COOKIE)
    df = spider.run()

    # --- 2. å®‰å…¨æ£€æŸ¥ ---
    if df.empty:
        print("\n" + "=" * 50)
        print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªèƒ½è·å–ä»»ä½•æ•°æ®ï¼")
        print("å¯èƒ½åŸå› ï¼š")
        print("1. MOVIE_ID å¡«é”™äº† (æ˜¯å¦å¡«äº†ç½‘å€è€Œéæ•°å­—ï¼Ÿ)")
        print("2. Cookie è¿‡æœŸæˆ–æœªå¡«å†™ (æ˜¯å¦çœ‹åˆ°äº† 'ç™»å½•' æç¤ºï¼Ÿ)")
        print("3. IP è¢«å° (æ˜¯å¦çœ‹åˆ°äº† 403 Forbiddenï¼Ÿ)")
        print("=" * 50)
        # å¼ºåˆ¶ç»“æŸï¼Œé˜²æ­¢åé¢æŠ¥é”™
        exit()

        # --- 3. åˆ†æä¸å±•ç¤º ---
    try:
        nlp = NLPProcessor('douban_comments.csv')
        nlp.process()

        viz = Visualizer('douban_processed.csv')
        viz.run_all()

        print("\nğŸ‰ æ­å–œï¼ç³»ç»Ÿè¿è¡ŒæˆåŠŸï¼Œè¯·åœ¨å½“å‰æ–‡ä»¶å¤¹æŸ¥çœ‹ç”Ÿæˆçš„å›¾ç‰‡ã€‚")
    except Exception as e:
        print(f"è¿è¡Œåˆ†ææ—¶å‡ºé”™: {e}")